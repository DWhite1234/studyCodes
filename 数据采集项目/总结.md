# 数据采集项目
    ## 1.日志数据
        ### 1.平台搭建
        1.hadoop
            注意点:
            1.配置了lzo压缩支持
            2.开启数据均衡 start-balancer.sh -threshold 10(相差率不超过10%)
            3.关闭数据均衡stop-balancer.sh
        2.zookeeper
        3.flume
            注意点:两个job
            1.flume-kafka.conf(taildir+kafka channel)
            2.kafka-hdfs.conf(kafka source+memory channel+hdfs sink)
            3.自定义拦截器
        4.kafka
            注意点:
            1.配置文件多了根目录/kafka
            2.机器数量的计算
            3.分区数量计算
        5.hive
        ### 2.测试数据通道
        ### 3.各个脚本
    ## 2.业务数据
        ### 1.sqoop安装
        ### 2.sqoop数据导入hdfs
        重点:
        1.Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用--input-null-string和--input-null-non-string两个参数。导入数据时采用--null-string和--null-non-string。
        2.
            $sqoop import \
            --connect jdbc:mysql://hadoop102:3306/$APP \
            --username root \
            --password 000000 \
            # 写到hdfs目录
            --target-dir /origin_data/$APP/db/$1/$do_date \
            # 如果重新传输,会覆盖以前的数据,配和ack实现数据一致性
            --delete-target-dir \
            --query "$2 and  \$CONDITIONS" \
            # 指定mapper Task的个数
            --num-mappers 1 \
            # 指定文件的分隔符
            --fields-terminated-by '\t' \
            # 指定压缩方式
            --compress \
            --compression-codec lzop \
            # mysql导入hdfs保证两端null一致,mysql null就是null,hdfs null是 \N
            --null-string '\\N' \
            # hdfs到处mysql保证两端null一致
            --null-non-string '\\N'
            # 指定使用lzo的包,创建索引
            hadoop jar /opt/module/hadoop-3.1.3/share/hadoop/common/hadoop-lzo-0.4.20.jar com.hadoop.compression.lzo.DistributedLzoIndexer /origin_data/$APP/db/$1/$do_date
