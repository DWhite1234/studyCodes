# 1.某些fetch操作可以不使用mr计算,如果select name form tb;
全局查找、字段查找、limit查找等都不走mapreduce
set hive.fetch.task.conversion=more (默认)

让所有的操作都走mr
set hive.fetch.task.conversion=none 

# 2.小数据集开启本地模式
开启本地模式
set hive.exec.mode.local.auto=true;

最大输入数据量,默认128M,小于这个数据量时,走本地模式
set hive.exec.mode.local.auto.inputbytes.max=250000000;

最大输入文件个数，当输入文件个数小于这个值时,走本地模式,默认为4
set hive.exec.mode.local.auto.input.files.max=10;

# 表的优化
## 1.小表,大表join
1.小表在左,大表在右(hive新版已经优化,现在已经无所谓)
2.使用map join,把小表读入内存
    设置自动选择Mapjoin
    set hive.auto.convert.join = true; 默认为true

    大表小表的阈值设置（默认25M以下认为是小表）
    set hive.mapjoin.smalltable.filesize=25000000;

## 2.大表jon大表
1.由于hive很容易造成NULL值,所以可能会由null引起 数据倾斜
NVL()或者where过滤null值

2.where子句先过滤在join,与先join在where过滤,hive已经优化,都是先过滤在join
select 
*
from(
select n.* from nullidtable n
left join bigtable o on n.id = o.id)
t1
where t1.id is not null


select n.* 
from (select * from nullidtable where id is not null ) n  
left join bigtable 
on n.id = o.id;

3.桶join
    1.两个分桶表
    2.设置参数
    set hive.optimize.bucketmapjoin = true;
    set hive.optimize.bucketmapjoin.sortedmerge = true;
    set hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;



## 3.group by分组导致的同一个key,数据倾斜
使用combiner,进行预聚合

## 4.partition导致的同一个分区,数据倾斜
对数据倾斜的分区,进行二次细分

## 5.不用distinct,因为这个操作是在一个reduce中执行的,会导致该reduce压力过大

## 6.避免笛卡尔积
 1.使用join的时候,一定不要忘记on条件
 2.设置属性不允许笛卡尔迪的查询运行
 set hive.strict.checks.cartesian.product=true

## 7.order by查询
设置属性set hive.strict.checks.orderby.no.limit=true
order by的查询必须使用limit,否则不允许执行

## 8.分区表不允许查询所有分区
设置属性set hive.strict.checks.no.partition.filter=true

## 9.开启并行job,但是极其消耗性能,慎用
打开任务并行执行，默认为false
set hive.exec.parallel=true;

同一个sql允许最大并行度，默认为8       
set hive.exec.parallel.thread.number=16;

## 10.合理设置map数和reduce数
1.小文件过多,使用combineTextInputFormat
set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;

2.mr的任务结束时,合并小文件
    map-only任务结束时合并小文件，默认true
    set hive.merge.mapfiles = true;

    map-reduce任务结束时合并小文件，默认false
    set hive.merge.mapredfiles = true;

    合并文件的大小，默认256M
    set hive.merge.size.per.task = 268435456;

    当输出文件的平均大小小于该值时，启动一个独立的map-reduce任务进行文件merge
    set hive.merge.smallfiles.avgsize = 16777216;

3.合理设置reduce数量
    每个Reduce处理的数据量默认是256MB
    set hive.exec.reducers.bytes.per.reducer=256000000 

    hadoop,mapred-default.xml设置每个job的reduce数
    set mapreduce.job.reduces = 15;   






