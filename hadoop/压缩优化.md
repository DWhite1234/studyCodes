# 1.压缩分类
```txt
压缩格式    是否hadoop自带  算法      拓展名     是否可切片    是否需要特殊处理

DEFLATE     是             DEFLATE  .DEFLATE   否              否
gzip        是             DEFLATE  .gz        否              否
bzip2       是             bzip2    .bzip2     是              否
LZo         否             LZO      .lzo       是              需要建立索引,指定输入格式
Snappy      是             Snappy   .snappy    否              否             
```

# 2.对应的编解码器
```txt
DEFLATE     org.apache.hadoop.io.compress.DefaultCodec
gzip        org.apache.hadoop.io.compress.GzipCodec
bzip2       org.apache.hadoop.io.compress.BZip2Codec
LZo         com.hadoop.compression.lzo.LzopCodec
Snappy      org.apache.hadoop.io.compress.SnappyCodec
```

# 3.压缩性能比较
```txt
             压缩倍率   压缩速度    解压缩速度
gzip           20%       20M          60M
bzip2          12.5%     2M           9M
LZo            30%       40M          80M
Snappy         50%       250M         500M
```

# 4.压缩对比总结(常用)
```txt
bzip2:
    压缩倍率极高,但是压缩解压缩速度慢,适用于不怎么使用的数据,减少空间占用
LZo:
    压缩倍率可以,压缩解压缩速度相比128M块大小,略微有影响,但是基本可以使用
Snappy:
    压缩倍率一般,压缩解压缩速度相比128M块大小,基本没有影响,可以使用在mapper输出
```

# 5.压缩的位置
1.mapper输入(hadoop自动会识别压缩文件,只需要开启识别压缩)
2.mapper输出(Snappy极佳)
3.reduce输出(看最终输出的数据使用情况,选择不同)

# 6.压缩在代码中的使用
1.手写压缩
```java
public class TestCompress {

    public static void main(String[] args) throws IOException {
        //压缩方法
//        compress("D:\\input\\word.txt","org.apache.hadoop.io.compress.BZip2Codec");
        //解压方法
        decompress("D:\\input\\word.txt.bz2","org.apache.hadoop.io.compress.BZip2Codec");
    }
    /*
        如果是解压缩,输出普通文件,就需要把普通输入流包装成压缩输入流,以普通流写出
    */
    private static void decompress(String path, String method) throws IOException {
        //获取编码器工厂
        CompressionCodecFactory codecFactory = new CompressionCodecFactory(new Configuration());
        //获取压缩编码器
        CompressionCodec codec = codecFactory.getCodecByName(method);
        //开启普通输入流
        FileInputStream fis  = new FileInputStream(path);
        //开启压缩输入流
        CompressionInputStream decompress = codec.createInputStream(fis);
        //开启普通输出流
        FileOutputStream fos = new FileOutputStream("D:\\input\\word.txt");
        //流的对拷
        IOUtils.copyBytes(decompress, fos, new Configuration());
        //关闭流
        IOUtils.closeStreams(fos,decompress,fis);
    }
    /*
        如果是压缩,输出压缩文件,就需要把普通输出流包装成压缩输出流
    */
    private static void compress(String fileName, String method) throws IOException {
        Configuration conf = new Configuration();
        //获取编码器工厂
        CompressionCodecFactory codecFactory = new CompressionCodecFactory(conf);
        //根据对应压缩方法的类,获取压缩编码器
        CompressionCodec codec = codecFactory.getCodecByName(method);
        //开启普通输入流
        FileInputStream fis = new FileInputStream(fileName);
        //开启普松输出流
        FileOutputStream fos = new FileOutputStream(new File(fileName+codec.getDefaultExtension()));
        //开启压缩输出流
        CompressionOutputStream compress = codec.createOutputStream(fos);
        //流的对拷
        IOUtils.copyBytes(fis, compress, conf);
        //关闭资源
        IOUtils.closeStreams(compress,fos, fis);
    }
}

```
2.在hadoop中使用
```java
//开启map的压缩输出
conf.set("mapreduce.map.output.compress", "true");
conf.set("mapreduce.map.output.compress.codec", "org.apache.hadoop.io.compress.BZip2Codec");

//开启reduce的压缩输出
conf.set("mapreduce.output.fileoutputformat.compress", "true");
conf.set("mapreduce.output.fileoutputformat.compress.codec", "org.apache.hadoop.io.compress.BZip2Codec");
```